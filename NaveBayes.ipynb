{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuoSjqqlSiqsN0qsd4kIZs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaksonPascoal/Algebra_Linear_Estudos/blob/main/NaveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Naive Bayes é uma família de algoritmos de classificação baseados no Teorema de Bayes, sendo particularmente úteis em problemas de classificação probabilística. O adjetivo \"naive\" (ingênuo) vem do fato de que esses algoritmos assumem uma independência condicional entre as variáveis, o que na prática raramente acontece, mas ainda assim, os algoritmos Naive Bayes podem ser extremamente eficazes. Vamos dividir a explicação em sua história, matemática por trás, variantes e aplicações práticas.\n",
        "\n",
        "1. História\n",
        "O Naive Bayes remonta ao desenvolvimento do Teorema de Bayes, nomeado após o matemático Thomas Bayes (1702–1761), que formulou o teorema fundamental sobre a probabilidade condicional. O teorema foi publicado postumamente em 1763 e estabelece uma maneira de atualizar as probabilidades à medida que novas evidências surgem. Embora o teorema tenha sido formulado no século XVIII, os algoritmos Naive Bayes só ganharam destaque com o desenvolvimento da aprendizagem de máquina e dos classificadores probabilísticos no século XX.\n",
        "\n",
        "A abordagem ingênua de Bayes começou a ser amplamente explorada em aprendizado de máquina e ciência da computação a partir das décadas de 1950 e 1960, principalmente em sistemas de detecção de spam e classificação de texto.\n",
        "\n",
        "2. Teoria e Matemática\n",
        "O Naive Bayes é baseado no Teorema de Bayes, que é dado pela fórmula:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n",
        "Onde:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "P(A∣B) é a probabilidade de\n",
        "𝐴\n",
        "A ocorrer dado que\n",
        "𝐵\n",
        "B ocorreu (probabilidade posterior).\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "P(B∣A) é a probabilidade de\n",
        "𝐵\n",
        "B dado\n",
        "𝐴\n",
        "A (probabilidade da verossimilhança).\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "P(A) é a probabilidade de\n",
        "𝐴\n",
        "A ocorrer independentemente de\n",
        "𝐵\n",
        "B (probabilidade a priori).\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(B) é a probabilidade de\n",
        "𝐵\n",
        "B ocorrer.\n",
        "O Naive Bayes usa esse princípio para classificar uma nova instância de dados\n",
        "𝑥\n",
        "x, atribuindo-a à classe\n",
        "𝐶\n",
        "𝑘\n",
        "C\n",
        "k\n",
        "​\n",
        "  que maximiza a probabilidade posterior\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(C\n",
        "k\n",
        "​\n",
        " ∣x). O classificador assume que todas as características\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,...,x\n",
        "n\n",
        "​\n",
        "  são condicionalmente independentes entre si, dada a classe\n",
        "𝐶\n",
        "𝑘\n",
        "C\n",
        "k\n",
        "​\n",
        " . Isso nos dá a fórmula:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "∝\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "∏\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "P(C\n",
        "k\n",
        "​\n",
        " ∣x)∝P(C\n",
        "k\n",
        "​\n",
        " )\n",
        "i=1\n",
        "∏\n",
        "n\n",
        "​\n",
        " P(x\n",
        "i\n",
        "​\n",
        " ∣C\n",
        "k\n",
        "​\n",
        " )\n",
        "3. Variantes do Naive Bayes\n",
        "Existem diferentes tipos de classificadores Naive Bayes, que variam dependendo do tipo de distribuição das variáveis independentes.\n",
        "\n",
        "Gaussian Naive Bayes: Usado quando as variáveis são contínuas e assume que as variáveis seguem uma distribuição normal (gaussiana).\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        "𝑘\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "𝜋\n",
        "𝜎\n",
        "𝑘\n",
        "2\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝜇\n",
        "𝑘\n",
        ")\n",
        "2\n",
        "2\n",
        "𝜎\n",
        "𝑘\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣C\n",
        "k\n",
        "​\n",
        " )=\n",
        "2πσ\n",
        "k\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "1\n",
        "​\n",
        " exp(−\n",
        "2σ\n",
        "k\n",
        "2\n",
        "​\n",
        "\n",
        "(x\n",
        "i\n",
        "​\n",
        " −μ\n",
        "k\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "​\n",
        " )\n",
        "Aqui\n",
        "𝜇\n",
        "𝑘\n",
        "μ\n",
        "k\n",
        "​\n",
        "  e\n",
        "𝜎\n",
        "𝑘\n",
        "2\n",
        "σ\n",
        "k\n",
        "2\n",
        "​\n",
        "  são a média e a variância da variável\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  para a classe\n",
        "𝐶\n",
        "𝑘\n",
        "C\n",
        "k\n",
        "​\n",
        " .\n",
        "\n",
        "Multinomial Naive Bayes: É usado principalmente para classificação de texto, onde as variáveis são contagens de palavras. O algoritmo assume que as características são distribuídas de acordo com uma distribuição multinomial.\n",
        "\n",
        "Bernoulli Naive Bayes: Também usado para classificação de texto, mas aqui as variáveis são binárias (ocorrência ou ausência de uma palavra em um documento). É adequado quando os atributos de entrada são booleanos.\n",
        "\n",
        "4. Uso Prático\n",
        "O Naive Bayes é popular devido à sua simplicidade, rapidez e eficiência, sendo utilizado principalmente em classificação de texto e filtragem de spam. Alguns de seus usos comuns incluem:\n",
        "\n",
        "Classificação de e-mails: Filtragem de spam é um dos principais casos de uso. O algoritmo pode determinar a probabilidade de um e-mail ser spam com base na frequência de certas palavras.\n",
        "Análise de sentimentos: Classificação de textos com base em suas características (positivas ou negativas), muito útil em reviews de produtos e redes sociais.\n",
        "Detecção de fraudes: O Naive Bayes pode ser utilizado em sistemas de detecção de fraudes, atribuindo probabilidades a comportamentos suspeitos.\n",
        "Recomendação de filmes ou produtos: Com base em classificações e preferências de usuários, ele pode inferir o que outro usuário pode gostar.\n",
        "5. Vantagens e Desvantagens\n",
        "Vantagens:\n",
        "Simples e rápido: O Naive Bayes é fácil de implementar e computacionalmente eficiente.\n",
        "Bom desempenho com muitos atributos: Mesmo quando há muitos atributos (como em problemas de classificação de texto), o Naive Bayes pode ser eficaz.\n",
        "Robustez com dados pequenos: Ele pode se sair bem, mesmo com pequenas quantidades de dados de treinamento.\n",
        "Desvantagens:\n",
        "Suposição de independência: A suposição de que os atributos são condicionalmente independentes pode ser irrealista em muitos cenários, o que pode prejudicar a precisão.\n",
        "Problemas com dados contínuos: Embora a variante gaussiana tente resolver isso, o Naive Bayes ainda pode não ser adequado para dados contínuos sem pré-processamento adequado.\n",
        "Zero frequency problem: Quando uma variável categórica em um novo dado não foi observada no conjunto de treinamento, o Naive Bayes atribui uma probabilidade zero, o que pode ser problemático. Isso é resolvido com a suavização de Laplace."
      ],
      "metadata": {
        "id": "PR3t6QKXnnL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NZb-QGxnb1w",
        "outputId": "31680038-f989-45ab-ffaa-9f9839bd7b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.98\n"
          ]
        }
      ],
      "source": [
        "#implementação do Nave Bayes com o dataset Iris\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Carregar dataset\n",
        "data_iris = load_iris()\n",
        "X = data_iris.data\n",
        "y = data_iris.target\n",
        "\n",
        "# Dividir em dados de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Criar o modelo Gaussian Naive Bayes\n",
        "model = GaussianNB()\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prever no conjunto de teste\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Acurácia: {accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_iris.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEJMsqxGo_-n",
        "outputId": "a502ee16-4d8b-4340-b1c8-87c08e02dfcd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_iris.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPg-C7N7pD8b",
        "outputId": "c82a6f66-f6be-4f55-ec45-c82ac2bd4527"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_iris.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS4E32UZptNL",
        "outputId": "491f8a86-993b-4474-deb2-b31a3699a993"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementação com o dataset cancer de mama da propria biblioteca scikit-learn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar novo dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Criar e treinar o modelo\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prever e avaliar\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Acurácia no novo dataset: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmLY6KReokqT",
        "outputId": "14cbee9b-4e44-416d-875a-9ff5a54a3e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia no novo dataset: 0.94\n"
          ]
        }
      ]
    }
  ]
}